# Worked Examples

These examples show what good skill output looks like. Use them as calibration anchors — not templates to copy, but demonstrations of the quality bar, triage logic, and coaching voice in action.

---

## Example 1: Scored Answer at Each Quality Level

**Question**: "Tell me about a time you had to make a difficult prioritization decision."

### Score 2 Answer (Weak)

> "We had a lot of projects going on and I had to figure out what to focus on. I looked at the data and decided to prioritize the most important ones. It worked out well and the team was happy with the results."

**Scores**: Substance 2 / Structure 2 / Relevance 2 / Credibility 1 / Differentiation 1
**Calibration band**: Mid-career

**Why these scores**:
- Substance 2: Vague claim ("looked at the data") with no specifics. What data? What projects? What was the trade-off?
- Structure 2: Has a beginning/middle/end but no narrative tension. Buries the actual decision.
- Relevance 2: Addresses the topic (prioritization) but doesn't answer what made it *difficult*.
- Credibility 1: No numbers, no specifics, no constraints. "It worked out well" — says who? How?
- Differentiation 1: Any candidate in any role could give this answer verbatim.

**Root cause pattern**: Conflict avoidance — the candidate stripped the story of all tension and difficulty, leaving a hollow shell.

---

### Score 3 Answer (Competent but Generic)

> "At my last company, I was managing three initiatives simultaneously — a new onboarding flow, a dashboard rebuild, and an API migration. We only had engineering bandwidth for two. I created a scoring framework based on user impact, engineering effort, and strategic alignment. The API migration scored lowest, so I deprioritized it. We shipped the onboarding flow and dashboard on time, and onboarding completion improved by 15%."

**Scores**: Substance 3 / Structure 3 / Relevance 3 / Credibility 3 / Differentiation 2
**Calibration band**: Mid-career

**Why these scores**:
- Substance 3: Specific claim with one metric (15%), but missing alternatives considered and the trade-off cost of deprioritizing the API migration.
- Structure 3: Clear STAR format but mechanical — setup → decision → result without narrative arc or tension.
- Relevance 3: Addresses the question but doesn't explain what made this *difficult*. Sounds like a straightforward framework application.
- Credibility 3: Has a number (15%) but no validation, no constraints mentioned, no acknowledgment of what was sacrificed.
- Differentiation 2: Uses a scoring framework — sounds like a PM textbook, not a specific person's judgment.

**Root cause pattern**: "Good enough" syndrome — specific enough to be credible, but stops before the interesting parts (the tension, the trade-off cost, the hard conversation with the API team).

---

### Score 4-5 Answer (Strong, Distinctive)

> "I was running three initiatives with bandwidth for two. The obvious cut was our API migration — lowest user-facing impact. But I'd learned from a previous mistake that deprioritizing infrastructure always compounds. Six months earlier, I'd made the 'user-first' call on a similar trade-off and we spent the next quarter firefighting tech debt.
>
> So instead of using a scoring framework, I went to the engineering lead and asked: 'If we delay the API migration by one quarter, what breaks?' Turns out, two partner integrations would stall, costing roughly $200K in committed revenue.
>
> I deprioritized the dashboard rebuild instead — controversial, because the VP of Sales had been promised it. I brought data to that conversation: the dashboard's primary users had built workarounds in Looker that covered 80% of the need. The remaining 20% could wait a quarter without measurable revenue impact.
>
> We shipped onboarding (+15% completion) and the API migration (preserving $200K in partner revenue). The dashboard shipped one quarter later with no measurable business impact from the delay. The earned lesson: prioritization frameworks are useful for the easy calls. For the hard ones, you need to understand the second-order cost of delay, not just the first-order impact of shipping."

**Scores**: Substance 5 / Structure 5 / Relevance 5 / Credibility 5 / Differentiation 4
**Calibration band**: Mid-career

**Why these scores**:
- Substance 5: Quantified impact ($200K, 15%, 80%), alternatives considered (dashboard vs. API), decision rationale with trade-offs, and outcome validated.
- Structure 5: Narrative arc with tension (the obvious call vs. the learned instinct), a twist (the counterintuitive choice), and a clean landing. Every sentence advances the story.
- Relevance 5: Directly addresses what made it *difficult* — it wasn't a framework problem, it was a judgment call where the "obvious" answer was wrong.
- Credibility 5: Numbers + constraints (VP promise) + realistic trade-offs (dashboard delay was manageable because of workarounds) + honest acknowledgment of a previous mistake.
- Differentiation 4: The earned secret ("prioritization frameworks work for easy calls, not hard ones") is genuine and specific. The previous-mistake callback shows self-awareness. One notch below 5 because the earned secret, while good, could be sharpened further.

---

## Example 2: Complete analyze Triage Decision in Action

**Context**: Mid-career PM, behavioral screen at a Series B startup. 6 questions in transcript.

**Aggregate scores**:
| Q# | Sub | Str | Rel | Cred | Diff |
|----|-----|-----|-----|------|------|
| 1  | 3   | 2   | 4   | 3    | 2    |
| 2  | 3   | 2   | 3   | 3    | 2    |
| 3  | 4   | 3   | 2   | 3    | 3    |
| 4  | 3   | 2   | 3   | 2    | 2    |
| 5  | 3   | 1   | 3   | 3    | 2    |
| 6  | 3   | 3   | 4   | 3    | 2    |
| **Avg** | **3.2** | **2.2** | **3.2** | **2.8** | **2.2** |

### Triage Decision

**Priority stack check**:
1. Relevance: Average 3.2. One answer at 2, rest at 3-4. Not the primary bottleneck.
2. Substance: Average 3.2. All at 3 except one 4. Not the primary bottleneck.
3. **Structure: Average 2.2. Four out of six answers below 3. This is the primary bottleneck.**
4. Credibility: Average 2.8. One answer at 2, rest at 3. Secondary concern.
5. Differentiation: Average 2.2. Five out of six below 3. Significant but lower priority than Structure.

**Multiple bottlenecks detected**: Structure AND Differentiation are both weak. Per priority stack, address Structure first — the candidate has content (Substance 3.2) but can't organize it under pressure. Differentiation work is premature until narrative architecture improves.

**Coaching path chosen**: Focus debrief on narrative architecture. Run constraint ladder drill immediately. Skip deep Calibration lens (the issue isn't word count or jargon — it's structure). Flag Differentiation as next priority after Structure reaches 3+.

**Psychological check**: No evidence of emotional bottleneck — scores are consistent between practice-like answers (no anxiety-driven variance visible in transcript).

**Debrief framing**: "Your content is solid — you have real experience and good examples. The issue is how you're packaging it. Four of your six answers lost their thread midway through. Let's focus entirely on narrative structure before we touch anything else. I want to run a constraint ladder drill on your weakest answer right now."

---

## Example 3: Practice Round Debrief with Self-Assessment Delta

**Drill**: Pushback drill (Stage 2)
**Question**: "Tell me about a time you disagreed with your manager."
**Interruption delivered**: At the 45-second mark — "I'm skeptical. It sounds like you just went along with their decision in the end."

### Round Debrief
- Drill: Pushback
- Objective: Maintain credibility and composure under skeptical challenge
- Candidate Self-Assessment: "I think I handled it okay — maybe a 3 on Credibility? I got a bit flustered by the interruption."

### What Worked
1. You acknowledged the skepticism directly instead of deflecting: "That's a fair read — let me clarify what actually happened." This is strong. [E:Practice round observation]
2. Your opening structure was clean — you front-loaded the disagreement before the context.

### Gaps
1. After the interruption, you over-explained. Your response to the pushback was 90 seconds — the pushback response should be 20-30 seconds max. You were defending, not clarifying. [E:Practice round observation]
2. The resolution was vague: "We eventually aligned." How? Who moved? What did you specifically do to influence the outcome?

### Scorecard
- Substance: 3 — Good setup, but the resolution lacks specifics.
- Structure: 3 — Strong opening, but the pushback response broke your structure.
- Relevance: 4 — Directly addressed disagreement and conflict.
- Credibility: 3 — The interruption response felt defensive rather than confident.
- Differentiation: 2 — This is a common "disagreed but aligned" story. What's the version only you could tell?

### Self-Assessment Delta
- Candidate rated themselves: ~3 on Credibility
- Coach scored: 3 on Credibility
- Calibration gap: Accurate on this one. Good self-awareness. (Note: across 4 prior rounds, candidate has consistently rated Structure 1 point higher than coach scores — this pattern hasn't surfaced here but is tracked in COACHING_STATE.)

### Next Round Adjustment
- Try this single change: When interrupted with pushback, respond in ONE sentence that acknowledges and redirects, then continue your story. Practice: "Fair point — here's what makes this different: [one sentence]. So what I did was..." No more than 15 seconds on the pushback response.

---

## Example 4: Answer Rewrite Showing the Delta

**Original answer** (scored Substance 3 / Differentiation 2):
> "I led the migration of our payment system from Stripe to an in-house solution. It was a complex project involving multiple teams. We planned it carefully, executed in phases, and completed it on time. The new system saved us about 30% on transaction fees."

**Rewrite at 4-5 quality** (with annotations):

> "I led our payment migration from Stripe to in-house — **[ADDED: tension]** which our CTO initially vetoed because the risk-to-reward ratio looked terrible on paper. **[ADDED: specific constraint]** We were processing $4M/month through Stripe, and any migration downtime would cost us roughly $130K per day.
>
> **[ADDED: what made YOUR approach different]** What changed his mind was a phased migration plan I built where we ran both systems in parallel for 60 days — Stripe as primary, our system processing shadow transactions. That let us validate accuracy before any customer saw the switch. **[ADDED: earned secret]** The counterintuitive lesson: the parallel run cost us an extra $80K in dual fees, but it eliminated the single biggest risk. Most engineers I've talked to try to do payment migrations as a cutover. That's how you get outages.
>
> **[PRESERVED: outcome, with specificity added]** We completed the migration with zero downtime across 3 phases over 4 months. Transaction fees dropped 32% — saving roughly $1.3M annualized. **[ADDED: validation]** Our CFO cited it in the board deck as the highest-ROI infrastructure investment that year."

**What changed and why**:
- Added the CTO veto → creates tension, shows influence skills, makes it a *story* not a report
- Added the $4M/month and $130K/day → makes the stakes concrete and credible
- Added the parallel-run approach → shows *how* you solved it, not just that you did
- Added the earned secret about parallel vs. cutover → this is differentiation; only someone who's done this migration knows this
- Added the CFO board deck mention → third-party validation that costs nothing to add but dramatically increases credibility
- Preserved the 30% fee reduction but made it specific (32%, $1.3M annualized) → approximations with context > round numbers

**Note**: The candidate would need to supply the actual numbers (the $4M, $130K, 32%, $1.3M, and CFO detail). The rewrite shows *where* specificity belongs and *what kind* of detail transforms a 3 into a 5. If they don't have exact numbers, approximations with caveats ("roughly $4M/month") are far better than no numbers at all.
