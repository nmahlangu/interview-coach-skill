# Role-Specific Drills

These drills simulate the scrutiny candidates face from specialists in their field. Run weekly, not just before interviews.

---

## Product Manager: Six-Lens Stress Test

**Setup**: Candidate describes a product decision or initiative they led.

**Challenge from 6 perspectives**, progressively harder based on performance:

### 1. Engineering Lens
- "This sounds like 6 months of work. How'd you scope it?"
- "What technical debt did this create?"
- "How'd you handle the infrastructure limitations?"
- "Walk me through the build vs. buy decision."
- "What did engineering push back on, and how did you resolve it?"

### 2. Design Lens
- "What user research backed this decision?"
- "How did you balance user needs vs. business goals?"
- "What did you sacrifice for simplicity?"
- "How many iterations did the design go through? What changed?"
- "What did users hate that you shipped anyway? Why?"

### 3. Data Lens
- "How did you measure success?"
- "What was your null hypothesis?"
- "How did you handle selection bias in your metrics?"
- "What metrics didn't move that you expected to?"
- "If I looked at your data, what would concern me?"

### 4. Business Lens
- "Walk me through the unit economics."
- "How'd this impact your growth loops?"
- "What was the opportunity cost of this vs. other projects?"
- "How did this affect revenue/retention/engagement?"
- "What would have happened if you'd done nothing?"

### 5. Competitor Lens
- "Competitor X tried this and failed. Why'd you succeed?"
- "How'd this affect your positioning?"
- "What stops them from copying this tomorrow?"
- "Who else considered this approach and abandoned it?"

### 6. Skeptic Lens
- "This seems like a solution looking for a problem."
- "Your success metrics feel cherry-picked."
- "How do you know this wasn't just regression to the mean?"
- "Sounds like you got lucky. What was actually skill?"
- "What would you do differently with hindsight?"

**Scoring per response:**
- Acknowledging the tension (vs. dismissing it): 1-5
- Specific evidence (vs. hand-waving): 1-5
- Admitting uncertainty (vs. false confidence): 1-5

---

## Software Engineer: Technical Depth Test

**Setup**: Candidate describes a technical project or system they built.

**Challenge across dimensions:**

### Architecture
- "Draw the system architecture. Where are the bottlenecks?"
- "What happens at 10x scale? 100x?"
- "What would you redesign if starting over?"
- "Where did you take shortcuts? What's the tech debt?"

### Trade-offs
- "Why this stack over alternatives?"
- "What did you optimize for? What did you sacrifice?"
- "How did you balance speed vs. quality?"
- "What's the maintenance burden of this approach?"

### Debugging
- "Walk me through the hardest bug you encountered."
- "How did you identify the root cause?"
- "What monitoring would have caught this earlier?"
- "How do you know it's actually fixed?"

### Collaboration
- "How did you handle disagreements on technical approach?"
- "How did you communicate technical constraints to non-engineers?"
- "What did you learn from code review feedback?"
- "How did you onboard others to this codebase?"

### Edge Cases
- "What happens when [component] fails?"
- "How do you handle [unusual input]?"
- "What's your rollback strategy?"
- "What security considerations did you address?"

**Scoring per response:**
- Technical accuracy: 1-5
- Depth of understanding (not just surface): 1-5
- Awareness of trade-offs and alternatives: 1-5

---

## Designer: Critique and Rationale Test

**Setup**: Candidate presents a design project (ideally with visuals, but verbal walkthrough works).

**Challenge across dimensions:**

### Research Foundation
- "What research informed this direction?"
- "How many users did you talk to? What surprised you?"
- "What did the data say that contradicted your intuition?"
- "How did you validate this solved the actual problem?"

### Design Rationale
- "Why this layout over alternatives?"
- "Walk me through your information hierarchy decisions."
- "What did you try that didn't work?"
- "How did you balance aesthetics vs. usability?"

### Constraints
- "What technical constraints shaped the design?"
- "How did you work within brand guidelines?"
- "What did you fight for that got cut?"
- "How did you handle stakeholder feedback you disagreed with?"

### Accessibility & Edge Cases
- "How does this work for users with [disability]?"
- "What happens on mobile? Slow connections?"
- "How does this scale with 10x content?"
- "What's the empty state? Error state?"

### Impact
- "How did you measure design success?"
- "What behavioral change did you observe?"
- "What would you improve in V2?"
- "How did this affect key metrics?"

**Scoring per response:**
- User-centeredness (vs. assumption-driven): 1-5
- Rationale clarity (can explain "why"): 1-5
- Openness to critique (vs. defensive): 1-5

---

## Data Scientist: Methodology Rigor Test

**Setup**: Candidate describes an analysis, model, or data project.

**Challenge across dimensions:**

### Problem Framing
- "How did you define the problem? Who defined success?"
- "What was the business question behind the technical question?"
- "What would 'wrong' look like? How would you know?"

### Data Quality
- "Where did the data come from? What's missing?"
- "How did you handle missing values? Outliers?"
- "What biases exist in this dataset?"
- "How representative is your sample?"

### Methodology
- "Why this approach over alternatives?"
- "What assumptions does this method require?"
- "How did you validate those assumptions?"
- "Walk me through your feature engineering decisions."

### Evaluation
- "What metrics did you optimize for? Why those?"
- "How did you prevent overfitting?"
- "What's your confidence interval? Statistical significance?"
- "How did you validate in production vs. test?"

### Communication
- "How did you explain this to non-technical stakeholders?"
- "What pushback did you get? How did you address it?"
- "What did you simplify for the audience? What did you lose?"

**Scoring per response:**
- Statistical rigor: 1-5
- Awareness of limitations: 1-5
- Business translation (not just technical): 1-5

---

## UX Researcher: Evidence and Influence Test

**Setup**: Candidate describes a research project and its impact.

**Challenge across dimensions:**

### Study Design
- "Why this method over alternatives?"
- "What's your sample size? How did you recruit?"
- "What biases might affect your findings?"
- "How did you ensure you weren't leading participants?"

### Analysis
- "How did you synthesize across participants?"
- "What patterns emerged? What outliers did you see?"
- "How did you distinguish signal from noise?"
- "What surprised you vs. confirmed expectations?"

### Insight Quality
- "What's the 'so what' of this finding?"
- "How actionable is this insight?"
- "What does this NOT tell us?"
- "How confident are you? What would change your mind?"

### Influence
- "How did you get stakeholders to act on this?"
- "Who disagreed? How did you handle it?"
- "What research did NOT lead to change? Why?"
- "How did you prioritize which findings to push?"

### Ethics
- "How did you handle sensitive participant data?"
- "What consent did participants give?"
- "Were there findings you chose not to share? Why?"

**Scoring per response:**
- Methodological soundness: 1-5
- Insight actionability: 1-5
- Stakeholder influence: 1-5

---

## Operations / Business Ops: Systems Thinking Test

**Setup**: Candidate describes a process, system, or operational improvement they led.

**Challenge across dimensions:**

### Problem Diagnosis
- "How did you identify this was the right problem to solve?"
- "What was the root cause vs. symptoms?"
- "What data told you this was worth fixing?"
- "What was the cost of doing nothing?"

### Solution Design
- "What alternatives did you consider?"
- "Why this approach over others?"
- "What dependencies did you uncover?"
- "How did you handle edge cases?"

### Implementation
- "How did you roll this out? Phased or all at once?"
- "What resistance did you encounter?"
- "How did you get buy-in from stakeholders?"
- "What broke during implementation?"

### Measurement
- "How did you measure success?"
- "What leading vs. lagging indicators did you track?"
- "How did you isolate impact from other changes?"
- "What didn't improve that you expected to?"

### Sustainability
- "How did you ensure this stuck after you moved on?"
- "What documentation/training did you create?"
- "Who owns this now?"
- "What maintenance burden did you create?"

**Scoring per response:**
- Systems thinking (sees connections): 1-5
- Change management awareness: 1-5
- Measurement rigor: 1-5

---

## Marketing: Strategy and Attribution Test

**Setup**: Candidate describes a campaign, launch, or marketing initiative.

**Challenge across dimensions:**

### Strategy
- "Why this channel/approach over alternatives?"
- "Who was the target audience? How did you define them?"
- "What was the competitive context?"
- "How did this fit into the broader marketing strategy?"

### Execution
- "Walk me through the timeline and key decisions."
- "What did you have to cut or change mid-flight?"
- "How did you work with creative/product/sales?"
- "What was your budget? How did you allocate it?"

### Attribution
- "How did you measure impact?"
- "What was your attribution model? What are its flaws?"
- "How did you separate this campaign's impact from other factors?"
- "What metrics didn't move that you expected to?"

### Learning
- "What would you do differently?"
- "What did you learn about the audience?"
- "How did this inform future campaigns?"
- "What surprised you about performance?"

**Scoring per response:**
- Strategic clarity: 1-5
- Measurement sophistication: 1-5
- Learning orientation: 1-5

---

## Interviewer Archetypes (for Panel Simulation)

When running `/practice panel` or `/mock` with panel format, deploy 2-3 of these distinct interviewer personas simultaneously. The candidate must learn to manage different interpersonal dynamics in one conversation.

### The Friendly Ally
- **Behavior**: Warm, encouraging, asks softball questions, builds rapport
- **Purpose**: Tests whether candidate stays rigorous when given easy questions, or becomes lazy/casual
- **Typical questions**: "That's fascinating — tell me more about that." / "What was the most rewarding part?"
- **What they're actually evaluating**: Depth of substance even when not pressured; whether warmth makes the candidate drop their guard
- **Danger for candidate**: Over-sharing, getting too casual, losing structure because the vibe is comfortable

### The Skeptic
- **Behavior**: Arms crossed, challenges every claim, asks "how do you know?" repeatedly
- **Purpose**: Tests defensiveness vs. intellectual curiosity under pressure
- **Typical questions**: "I'm not convinced — what evidence do you have?" / "That sounds like correlation, not causation." / "Your success metrics feel cherry-picked."
- **What they're actually evaluating**: Can you engage with skepticism without getting defensive? Do you acknowledge limitations?
- **Danger for candidate**: Getting defensive, over-explaining, losing confidence, or caving too quickly

### The Silent Observer
- **Behavior**: Takes notes, rarely speaks, minimal facial expressions, occasional nod
- **Purpose**: Tests composure without feedback; creates discomfort that reveals nerves
- **Typical questions**: Asks one or two precise, surgical questions late in the conversation
- **What they're actually evaluating**: How does the candidate perform without validation? Do they fill silence with rambling?
- **Danger for candidate**: Over-talking to fill the silence, getting anxious about lack of feedback, directing all attention to the more responsive panelists

### The Off-Script Wanderer
- **Behavior**: Asks unexpected questions unrelated to standard prep, goes on tangents, seems distracted
- **Purpose**: Tests adaptability and real-time thinking
- **Typical questions**: "Forget the project — what do you think about [industry trend]?" / "If you could redesign [company product] from scratch, what would you change?" / "What's something you believe that most people in your field disagree with?"
- **What they're actually evaluating**: Can you think on your feet? Do you have depth beyond prepared stories? Do you have genuine intellectual curiosity?
- **Danger for candidate**: Freezing on unexpected questions, trying to redirect back to prepared material instead of engaging

### The Time-Pressured Exec
- **Behavior**: Checks watch, speaks quickly, interrupts, wants bottom-line answers
- **Purpose**: Tests ability to compress and prioritize under time pressure
- **Typical questions**: "Give me the 30-second version." / "Skip the background — what was the outcome?" / "I have 5 minutes left — what's the one thing you want me to know?"
- **What they're actually evaluating**: Clarity under pressure. Can you get to the point? Do you know what matters?
- **Danger for candidate**: Rambling, starting with context instead of conclusions, failing to read the urgency signal

### The Culture Guardian
- **Behavior**: Friendly but probing on values, team dynamics, and how you treat people
- **Purpose**: Tests cultural fit and interpersonal judgment
- **Typical questions**: "Tell me about a time you disagreed with your manager." / "How did you handle someone on your team who was underperforming?" / "What kind of team culture do you create?"
- **What they're actually evaluating**: Empathy, self-awareness, conflict resolution style, whether your values actually align
- **Danger for candidate**: Giving "right" answers that sound performative, not demonstrating authentic values

### Panel Simulation Protocol

When running a panel drill:
1. Assign 2-3 archetypes (vary the combination each time).
2. Switch between personas naturally — don't announce "now the skeptic will ask."
3. Create moments where two interviewers' styles conflict (e.g., the Ally is encouraging while the Skeptic challenges the same point).
4. Debrief: How did the candidate manage competing dynamics? Did they play to one persona and neglect another? Did they read the room?

---

## General Drill Guidelines

1. **Start easier, escalate**: Begin with straightforward questions, increase difficulty based on how well they handle early ones.

2. **Push on "we"**: When candidate says "we," ask "What was YOUR specific contribution?"

3. **Probe the negative**: Every project has things that went wrong. If they only share positives, dig.

4. **Test adaptability**: Throw a curveball mid-answer: "Actually, let me ask about a different aspect..."

5. **Score in real-time**: Keep notes on where they're strong vs. struggling for debrief.

6. **Debrief with coaching presence**: After each drill round:
   - Ask: "How did that feel? What would you change if you could do it again?"
   - Reflect back what you observed working well FIRST
   - Share observations on gaps as questions when possible: "I noticed you didn't mention the data behind that decision — was that a deliberate choice, or did it slip?"
   - Close with: "What's one thing you want to try differently on the next round?"

7. **Notice patterns across drills**: If someone consistently struggles with a particular challenge type (e.g., always gets defensive on skeptical pushback), name the pattern: "I'm seeing this come up again. What do you think is happening when you get that kind of question?"

8. **Celebrate growth within a session**: When they improve from round 1 to round 3, name it specifically: "Did you notice the difference there? Your answer on that last one was half the length and twice as clear. What changed?"
